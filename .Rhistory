n.items = 6,
min.score.item = 1,
max.score.item = 7,
conf.level = .95)
output.MF
check.plot(output.MF)
check.plot(output.MF, color.band = T)
data.MF <- data.frame(matrix(sample(1:5, 100 * 10, replace = TRUE), ncol = 10))
data.MF
check.split(data = data.MF)
output.MF <- MF.CSEM(half1 = 1:3,
half2 = 4:6,
data = random_data3,
reliability.coef = .83,
n.items = 10,
min.score.item = 1,
max.score.item = 7,
conf.level = .95)
output.MF
check.plot(output.MF, color.band = T)
data.MF <- data.frame(matrix(sample(3:5, 100 * 10, replace = TRUE), ncol = 10))
data.MF
check.split(data = data.MF)
half.names <- check.split(data = data.MF)
half.names$half1
output.MF <- MF.CSEM(half1 = half.names$half1,
half2 = half.names$half1,
data = data.MF,
reliability.coef = .83,
n.items = 10,
min.score.item = 1,
max.score.item = 7,
conf.level = .95)
output.MF
check.plot(output.MF, color.band = T)
output.MF <- MF.CSEM(half1 = half.names$half1,
half2 = half.names$half2,
data = data.MF,
reliability.coef = .83,
n.items = 10,
min.score.item = 1,
max.score.item = 7,
conf.level = .95)
output.MF
check.plot(output.MF, color.band = T)
check.plot
output.MF <- MF.CSEM(half1 = half.names$half1,
half2 = half.names$half2,
data = data.MF,
reliability.coef = .83,
n.items = 10,
min.score.item = 1,
max.score.item = 5,
conf.level = .95)
output.MF
check.plot(output.MF, color.band = T)
check.plot(output.MF, plot.type = "band")
check.plot(output.MF, plot.type = "band", line.type = "blue")
check.plot(output.MF, score.type = "observed",plot.type = "band", line.type = "lightblue")
check.plot
check.plot(MF.CSEM.output = output.MF,
score.type = "observed",
plot.type = "CSEM")
check.plot(MF.CSEM.output = output.MF,
score.type = "observed",
plot.type = "CI")
check.plot(MF.CSEM.output = output.MF,
score.type = "observed",
plot.type = "band")
check.plot(MF.CSEM.output = output.MF,
score.type = "observed",
plot.type = "CI")
check.plot(MF.CSEM.output = output.MF,
score.type = "observed",
plot.type = "CSEM")
mirt::plot
showMethods(mirt::plot)
#' Distribution Tests: Anderson-Darling, Overlapping Index, Kendall's W
#'
#' @param half1 First half of the items
#' @param half2 Second half of the items
#' @param B Number of bootstrap resamples (default = 2000)
#' @param conf Confidence level for the confidence interval (default = 0.95)
#'
#' @return Data frame with the results of the distribution tests
#'
#' @export
check.distribution <- function(half1, half2, B = 2000, conf = 0.95) {
# Calculate total scores
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
n <- length(total1)
# 1️⃣ Anderson-Darling Test (only version 1)
ad.test <- kSamples::ad.test(total1, total2)
ad.stat <- ad.test$ad[1, 1]  # Column 1: AD statistic
ad.p <- ad.test$ad[1, 3]     # Column 3: p-value
# Calculate experimental effect size (ES)
es.ad <- (ad.stat / sqrt(n)) / log(n + 1)
# 2️⃣ Overlapping Index (OVI) with bootstrap confidence interval
ovi.result <- calculate.OVI(total1, total2, B = B, conf = conf)
# 3️⃣ Kendall's W with bootstrap confidence interval
kendall.res <- rcompanion::kendallW(t(rbind(total1, total2)), ci = TRUE, R = B, type = "perc", conf = conf)
kendall.val <- kendall.res$W
ci.kendall <- c(kendall.res$lower.ci, kendall.res$upper.ci)
# Create final dataframe
distribution.tests <- data.frame(
Test = c("Anderson-Darling Test", "Overlapping Index (OVI)", "Kendall W"),
Statistic = c(round(ad.stat, 3), round(ovi.result$OVI, 3), NA),
p.value = c(round(ad.p, 5), NA, NA),
CI.lower = c(NA, ovi.result$CI.lower, ci.kendall[1]),
CI.upper = c(NA, ovi.result$CI.upper, ci.kendall[2]),
ES = c(round(es.ad, 3), round(ovi.result$OVI, 3), round(kendall.val, 3))
)
return(distribution.tests)
}
data.MF[, half.names$half1]
check.distribution(half1 = data.MF[, half.names$half1], half2 = data.MF[, half.names$half2])
#' x <- rnorm(100, mean = 5, sd = 2)
#' y <- rnorm(100, mean = 5.5, sd = 2)
#' calculate.OVI(x, y, B = 500, conf = 0.95)
#'
#' # Example with overlapping distributions
#' x <- rnorm(100, mean = 0, sd = 1)
#' y <- rnorm(100, mean = 0, sd = 1)
#' calculate.OVI(x, y)
#'
#' @export
calculate.OVI <- function(x, y, B = 500, conf = 0.95) {
# Calculate the observed OVI
observed.OVI <- overlapping::overlap(list(x, y))$OV
# Bootstrap function
ovi.fun <- function(data, indices) {
sample1 <- data[indices, 1]
sample2 <- data[indices, 2]
overlapping::overlap(list(sample1, sample2))$OV
}
# Prepare data for bootstrapping
data.ovi <- cbind(x, y)
# Run bootstrap and handle potential errors
boot.result <- tryCatch({
boot::boot(data.ovi, ovi.fun, R = B)
}, error = function(e) NULL)  # Return NULL if bootstrap fails
# Calculate confidence interval if bootstrap was successful
if (is.null(boot.result)) {
ci <- c(NA, NA)
message("⚠️ Bootstrap failed; returning NA for confidence intervals.")
} else if (all(boot.result$t == boot.result$t0)) {
# All bootstrap samples are identical; cannot calculate CI
ci <- c(NA, NA)
message("⚠️ Bootstrap samples are identical; confidence intervals cannot be calculated.")
} else {
ci <- boot::boot.ci(boot.result, conf = conf, type = "perc")$percent[4:5]
}
# Round results to 3 decimal places
result <- list(
OVI = round(observed.OVI, 3),
CI.lower = ifelse(!is.na(ci[1]), round(ci[1], 3), NA),
CI.upper = ifelse(!is.na(ci[2]), round(ci[2], 3), NA)
)
return(result)
}
check.distribution(half1 = data.MF[, half.names$half1], half2 = data.MF[, half.names$half2])
calculate.OVI(x = rowSums(data.MF[, half.names$half1]),
y = rowSums(data.MF[, half.names$half2]), B = 200,conf = .95)
check.location <- function(half1, half2, B = 2000, conf = 0.95) {
# Calcular puntuaciones totales
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
n <- length(total1)
## 1️⃣ Prueba de Wilcoxon
combined <- c(total1, total2)
g <- factor(rep(c("Half1", "Half2"), each = n))
# Calcular coeficiente rc con IC
rc_result <- rcompanion::wilcoxonPairedRC(
x = combined,
g = g,
ci = TRUE,
R = B,
type = "perc",
conf = conf
)
wilcox_test <- wilcox.test(total1, total2, paired = TRUE, exact = FALSE)
## 2️⃣ Prueba robusta: Yuen's T-test
yuen_test <- PairedData::yuen.t.test(total1, total2, alternative = "two.sided", paired = TRUE, conf.level = conf)
effect_robust <- WRS2::dep.effect(x = total1, y = total2)
# Extraer resultados usando nombres
akp <- effect_robust[1, c("Est", "ci.low", "ci.up")]
# Acceder correctamente a los nombres
ci_low <- akp["ci.low"]
ci_up <- akp["ci.up"]
est_akp <- akp["Est"]
## 📊 Construir dataframe final
location_tests <- data.frame(
Test = c("Wilcoxon Test", "Yuen Robust Test"),
Statistic = c(wilcox_test$statistic, yuen_test$statistic),
p_value = c(wilcox_test$p.value, yuen_test$p.value),
CI_lower = c(rc_result$lower.ci, ci_low),
CI_upper = c(rc_result$upper.ci, ci_up),
ES = c(rc_result$rc, est_akp)
)
return(location_tests)
}
check.location(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2], B = 100, conf = .95)
check.scale <- function(half1, half2, conf = 0.95) {
# Calcular puntuaciones totales
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
# Calcular prueba Bonett-Seier para varianzas
bs_test <- PairedData::bonettseier.Var.test(total1, total2, conf.level = conf)
# Calcular log‐transformed variability ratio (lnVR) con metafor
lnvr <- metafor::escalc(measure = "VRC",
sd1i = sd(total1),
sd2i = sd(total2),
ri = cor(total1, total2),
ni = NROW(total1))
# Resumen para obtener IC
lnvr_summary <- summary(lnvr)
# Extraer resultados relevantes
scale_tests <- data.frame(
Test = "Bonett-Seier Test (con lnVR)",
Statistic = round(bs_test$statistic, 3),
p_value = round(bs_test$p.value, 5),
CI_lower = round(lnvr_summary$ci.lb, 3),
CI_upper = round(lnvr_summary$ci.ub, 3),
ES = round(lnvr_summary$yi, 3)
)
return(scale_tests)
}
check.scale <- function(half1, half2, conf = 0.95) {
# Calcular puntuaciones totales
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
# Calcular prueba Bonett-Seier para varianzas
bs_test <- PairedData::bonettseier.Var.test(total1, total2, conf.level = conf)
# Calcular log‐transformed variability ratio (lnVR) con metafor
lnvr <- metafor::escalc(measure = "VRC",
sd1i = sd(total1),
sd2i = sd(total2),
ri = cor(total1, total2),
ni = NROW(total1))
# Resumen para obtener IC
lnvr_summary <- summary(lnvr)
# Extraer resultados relevantes
scale_tests <- data.frame(
Test = "Bonett-Seier Test (con lnVR)",
Statistic = round(bs_test$statistic, 3),
p_value = round(bs_test$p.value, 5),
CI_lower = round(lnvr_summary$ci.lb, 3),
CI_upper = round(lnvr_summary$ci.ub, 3),
ES = round(lnvr_summary$yi, 3)
)
return(scale_tests)
}
check.scale <- function(half1, half2, conf = 0.95) {
# Calcular puntuaciones totales
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
# Calcular prueba Bonett-Seier para varianzas
bs_test <- PairedData::bonettseier.Var.test(total1, total2, conf.level = conf)
# Calcular log‐transformed variability ratio (lnVR) con metafor
lnvr <- metafor::escalc(measure = "VRC",
sd1i = sd(total1),
sd2i = sd(total2),
ri = cor(total1, total2),
ni = NROW(total1))
# Resumen para obtener IC
lnvr_summary <- summary(lnvr)
# Extraer resultados relevantes
scale_tests <- data.frame(
Test = "Bonett-Seier Test (con lnVR)",
Statistic = round(bs_test$statistic, 3),
p_value = round(bs_test$p.value, 5),
CI_lower = round(lnvr_summary$ci.lb, 3),
CI_upper = round(lnvr_summary$ci.ub, 3),
ES = round(lnvr_summary$yi, 3)
)
return(scale_tests)
}
check.scale(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
check.scale(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
conf = .95)
#' @title  Calculate Cronbach's Alpha for Two Halves
#' @title  Calculate Cronbach's Alpha for Two Halves
#'
#' @param half1 First half of the items
#' @title  Calculate Cronbach's Alpha for Two Halves
}
}
}
return(result)
}
#'
#'
#' @references
#' Cronbach, L. J. (1951). Coefficient alpha and the internal structure of tests.
#'  Psychometrika, 16, 297-334.
#'
#' @seealso
#' \code{\link[lavaan]{cfa}}, \code{\link[scripty]{mimic}}
#'
#' @export
check.alpha <- function(half1, half2, B = 1000, conf = 0.95) {
# Calculate Cronbach's alpha for a given half
cronbach.alpha <- function(x) {
k <- ncol(x)
r_avg <- mean(cor(x, use = "pairwise.complete.obs"))
(k * r_avg) / (1 + (k - 1) * r_avg)
}
# Calculate alpha for half1
alpha1 <- cronbach.alpha(half1)
# Bootstrap for confidence interval for alpha(half1)
boot.alpha1 <- boot::boot(half1, statistic = function(d, i) {
cronbach.alpha(d[i, ])
}, R = B)
ci1 <- tryCatch({
boot::boot.ci(boot.alpha1, conf = conf, type = "perc")$percent[4:5]
}, error = function(e) c(NA, NA))
# Calculate alpha for half2
alpha2 <- cronbach.alpha(half2)
# Bootstrap for confidence interval for alpha(half2)
boot.alpha2 <- boot::boot(half2, statistic = function(d, i) {
cronbach.alpha(d[i, ])
}, R = B)
ci2 <- tryCatch({
boot::boot.ci(boot.alpha2, conf = conf, type = "perc")$percent[4:5]
}, error = function(e) c(NA, NA))
# Return results
result <- data.frame(
Half = c("Half 1", "Half 2"),
Coefficient = "alpha",
Estimate = round(c(alpha1, alpha2), 4),
CI.lower = c(ci1[1], ci2[1]),
CI.upper = c(ci1[2], ci2[2])
)
return(result)
}
check.alpha(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
#' Calculate Angoff Coefficient
#'
#' @param half1 First half of the items
#' @param half2 Second half of the items
#' @param B Number of bootstrap resamples (default = 1000)
#' @param conf Confidence level for the confidence interval (default = 0.95)
#'
#' @return Data frame with the Angoff coefficient and confidence interval
#'
#' @export
check.angoff <- function(half1, half2, B = 1000, conf = 0.95) {
# Calculate total scores
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
# Calculate Angoff coefficient using the original formula
cov.val <- cov(total1, total2, use = "complete.obs")
var1 <- var(total1, na.rm = TRUE)
var2 <- var(total2, na.rm = TRUE)
# Apply the original formula
angoff.val <- 4 * cov.val / (var1 + var2 + 2 * cov.val)
# Bootstrap for confidence interval
boot.angoff <- boot::boot(data = cbind(total1, total2),
statistic = function(data, i) {
t1 <- data[i, 1]
t2 <- data[i, 2]
cov.val <- cov(t1, t2, use = "complete.obs")
var1 <- var(t1, na.rm = TRUE)
var2 <- var(t2, na.rm = TRUE)
4 * cov.val / (var1 + var2 + 2 * cov.val)
}, R = B)
ci <- tryCatch({
boot::boot.ci(boot.angoff, conf = conf, type = "perc")$percent[4:5]
}, error = function(e) c(NA, NA))
# Return results
result <- data.frame(
Coefficient = "Angoff Coefficient",
Estimate = round(angoff.val, 4),
CI.lower = ci[1],
CI.upper = ci[2]
)
return(result)
}
check.angoff(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
check.spearmanbrown <- function(half1, half2, B = 1000, conf = 0.95) {
# Calcular las puntuaciones totales
total1 <- rowSums(half1, na.rm = TRUE)
total2 <- rowSums(half2, na.rm = TRUE)
# Calcular el coeficiente de Spearman-Brown
r_halves <- cor(total1, total2, use = "complete.obs")
sb_reliability <- (2 * r_halves) / (1 + r_halves)
# Bootstrap para IC
boot_sb <- boot::boot(data = cbind(total1, total2),
statistic = function(data, i) {
t1 <- data[i, 1]
t2 <- data[i, 2]
r <- cor(t1, t2, use = "complete.obs")
(2 * r) / (1 + r)
},
R = B)
ci <- tryCatch({
boot::boot.ci(boot_sb, conf = conf, type = "perc")$percent[4:5]
}, error = function(e) c(NA, NA))
## 📦 Devolver resultados
result <- data.frame(
Coefficient = "Spearman-Brown",
Estimate = round(sb_reliability, 4),
CI_lower = ci[1],
CI_upper = ci[2]
)
return(result)
}
check.spearmanbrown(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
check.alpha(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
check.scale(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
conf = .95)
check.distribution(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
calculate.OVI(x = rowSums(data.MF[, half.names$half1]),
y = rowSums(data.MF[, half.names$half2]),
B = 100,
conf = .95)
check.omega <- function(half1, half2, B = 1000, conf = 0.95) {
# Función interna para calcular Omega usando lavaan
calculate_omega_lavaan <- function(data) {
# Definir modelo unifactorial dinámico
items <- colnames(data)
model <- paste0("Factor =~ ", paste(items, collapse = " + "))
# Ajustar el modelo con lavaan
fit <- tryCatch(
lavaan::cfa(model, data = data, estimator = "ulsmv", ordered = FALSE, std.lv = TRUE),
error = function(e) return(NULL)
)
if (is.null(fit)) return(NA)
# Extraer cargas factoriales
lambda <- lavaan::inspect(fit, "std")$lambda
communalities <- rowSums(lambda^2)
unique_vars <- 1 - communalities
# Calcular Omega
omega <- sum(lambda)^2 / (sum(lambda)^2 + sum(unique_vars))
return(omega)
}
## 1️⃣ Calcular Omega para half1
omega1 <- calculate_omega_lavaan(half1)
# Bootstrap para IC de Omega (Half1)
boot_omega1 <- boot::boot(data = half1, statistic = function(d, i) {
calculate_omega_lavaan(d[i, ])
}, R = B)
ci1 <- tryCatch({
boot::boot.ci(boot_omega1, conf = conf, type = "perc")$percent[4:5]
}, error = function(e) c(NA, NA))
## 2️⃣ Calcular Omega para half2
omega2 <- calculate_omega_lavaan(half2)
# Bootstrap para IC de Omega (Half2)
boot_omega2 <- boot::boot(data = half2, statistic = function(d, i) {
calculate_omega_lavaan(d[i, ])
}, R = B)
ci2 <- tryCatch({
boot::boot.ci(boot_omega2, conf = conf, type = "perc")$percent[4:5]
}, error = function(e) c(NA, NA))
## 📦 Devolver resultados
result <- data.frame(
Half = c("Half 1", "Half 2"),
Coefficient = "Omega total",
Estimate = round(c(omega1, omega2), 4),
CI_lower = c(ci1[1], ci2[1]),
CI_upper = c(ci1[2], ci2[2])
)
return(result)
}
check.omega(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
B = 100,
conf = .95)
devtools::document()
devtools::document()
check.scale(half1 = data.MF[, half.names$half1],
half2 = data.MF[, half.names$half2],
conf = .95)
devtools::document()
devtools::load_all()
?calc.adjusted.diff
example(calc.adjusted.diff)
help("calc.adjusted.diff")
rmarkdown::render("README.Rmd")
example("check.location")
unlink("man", recursive = TRUE) # Borra la documentación antigua
devtools::document()  # Genera la nueva documentación
devtools::load_all()  # Carga las funciones del paquete
devtools::document()  # Genera la nueva documentación
devtools::document()  # Genera la nueva documentación
unlink("man", recursive = TRUE) # Borra la documentación antigua
unlink("man", recursive = TRUE) # Borra la documentación antigua
devtools::document()  # Genera la nueva documentación
devtools::document()  # Genera la nueva documentación
dir.create("data")
